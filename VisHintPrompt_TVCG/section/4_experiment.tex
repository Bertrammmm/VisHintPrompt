% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=\columnwidth, alt={XXXX.}]{figs/pipeline.png}
%   \caption{%
%   	The pipeline of our \toolName{}.
%   }
%   \label{fig:piplineFig}
% \end{figure}

\section{Experiments}
\label{sec:experiments}
\subsection{Experimental Setup}

% Existing chart datasets often suffer from overly simplified and homogeneous visual structures, resulting in insufficient richness and complexity. 
% To enhance the visual understanding capability of MLLMs, datasets such as \textit{ChartQA}\cite{ChartQA} introduce specific numerical annotations into charts. However, the explicit presence of numerical values may bias the model toward textual cues and hinder its ability to align visual structures with semantics. In contrast, \textit{PlotQA}\cite{PlotQA} does not add specific numerical annotations, but its coverage of chart types is too limited, which leads to a lack of overall diversity. Although some datasets, for example \textit{ChartQAPro}\cite{ChartQAPro}, \textit{ChartX}\cite{ChartX}, \textit{ChartLlama}\cite{Chartllama}, and \textit{EvoChart}\cite{EvoChart}, offer a more comprehensive coverage of chart types, they still lack the most critical visual localization information for our study—namely, tick marks.
\subsubsection{\textbf{Evaluation Datasets}}
% To assess the effectiveness of our proposed \toolName{}, we evaluate it on dataset that generated by ourself across various chart types, including bar, line, scatterplot, bubble, pie, donut, radar, and rose charts. We introduce multi-dimensional visual parameters for each chart type, such as bar orientation, line style, label position, sector spacing, and area fill patterns, to ensure the diversity that aligned with the real-world charts, specifically as shown in \cref{tab:chart_types}. We know that there exists several benchmarks used for chart data extraction or chartQA, icluding ChartQA, PlotQA, and ExcelChart400K collected in ChartOCR, however, chart imaes in ChartQA most expose the data value on the original image, PlotQA only includes the scatterplot that weak in chart diversity, in the same time, ExcelChart400K collected in ChartOCR only include bar, line, and pie charts. In addition, ecxcept for the evalution for data extraction, we also have to evaluate the effectiveness of axis priors extraction, we generate our own dateset, that extend the chart types to include cartesian and polar axis, including  bar, line, scatterplot, bubble, pie, donut, radar, and rose charts, that record the axis priors information and ground data values. To augment the diversity of our datasets to more align with the real-world charts, we further introduced variations in canvas size, resolution, color schemes, font sizes, and the number of entities, thereby simulating the visual complexity and diversity encountered in real-world scenarios as faithfully as possible.
To rigorously evaluate \toolName{}, we construct a unified evaluation dataset spanning two coordinate families: Cartesian charts, including vertical and horizontal bar, line, scatterplot, and bubble charts, and polar charts, including pie, donut, radar, and rose charts. Each chart is paired with pixel-accurate ground truth values and pixel-level representations of axis priors, enabling joint evaluation of numerical value extraction and axis-aware understanding. Existing benchmarks such as ChartQA\cite{ChartQA}, PlotQA\cite{PlotQA}, and ExcelChart400K\cite{ChartOCR} are mainly designed for chart question answering or chart OCR tasks. For our task setting, differences in three aspects make these benchmarks difficult to use: limited chart type coverage, the presence of explicit numeric labels within charts, and the lack of ground truth for both underlying ground truth of data values 
% and pixel-level axis priors 
needed to evaluate numerical inference across diverse chart families. 
% To rigorously evaluate \toolName{}, we construct a comprehensive dataset that spans two coordinate families: Cartesian charts (bar, line, scatter, and bubble) and polar charts (pie, donut, radar, and rose).
% To rigorously evaluate \toolName{}, we construct a comprehensive
% dataset spanning a wide range of visualization
% styles and axis systems. \wy{1. axis systems? 2. ``A wide range of'' may be an overclaim, and we can explicitly mention the exact number of visualization types instead. } 

% Existing benchmarks such as ChartQA\cite{ChartQA}, PlotQA\cite{PlotQA},
% and ExcelChart400K\cite{ChartOCR} are mainly designed for chart question answering
% %
% or chart data extraction. \wy{Do we want to emphasize this? Our task is also chart data extraction, so the logic flow here is a bit weird.}
% %
% They either expose explicit numeric labels in the chart
% (ChartQA), provide limited chart diversity such as only scatterplots
% (PlotQA), or cover only bar, line, and pie charts (ExcelChart400K). 
% None of these datasets provides axis-prior ground truth, nor do they
% cover a sufficiently diverse set of chart types, especially those in
% the polar coordinate system. These two limitations prevent them from
% supporting the axis-aware and multi-type evaluation required by our
% visual hint prompting strategy.
% \wy{One possible missing point here is the availability of the ground truth value of the underlying data, which is crucial for evaluating the performance of our approach.}
% We construct a unified evaluation dataset that
% spans five Cartesian chart types (vertical bar, horizontal bar, line, scatter, and bubble) and four polar chart types (pie, donut, radar, and rose). Each chart is paired with pixel-accurate ground truth data values and axis-prior annotations, enabling joint evaluation of value extraction and axis understanding. 
As shown in Table \ref{tab:chart_types}, the dataset contains
450 charts (50 per type), and each chart includes several visual
targets such as bars, points, or sectors. In total, this yields over 4,000 point-level prediction instances, providing sufficient statistical stability for per-type evaluation while keeping the computational cost of multi-round MLLM inference tractable.
To approximate the visual and structural diversity of real-world
graphics, we introduce controlled variations in canvas size, image
resolution, color palettes, font families and sizes, label layouts,
object counts, and chart-specific properties such as bar orientation,
line and marker styles, bubble label positions, sector spacing, ring
widths, and radar dimensions. All charts are programmatically rendered
within a unified pipeline; the “Real” and “Synthetic” labels in Table
\ref{tab:chart_types} refer only to the provenance of the underlying
numeric data rather than to visual rendering. Real charts are generated
from real-world numeric tables, whereas synthetic charts rely on
randomly sampled distributions with controlled statistical patterns.
This design ensures consistent visual rendering while preserving
semantic variability across data sources. Table \ref{tab:chart_types} summarizes the chart categories, real or synthetic composition, and key style variations incorporated in our dataset, which is available at \href{https://github.com/tu4nzii/VisHintPrompt_datasets}{https://github.com/tu4nzii/VisHintPrompt\_datasets}. 
% \wy{1. Do we plan to publish the dataset? It will be good if we can; 2. It should be ``Table I'', NOT ``table I''. Please revise it accordingly.}



\begin{table}[htbp]
\centering
\caption{Overview of Chart Types and Data Composition}
\label{tab:chart_types}
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}llrrrl@{}}
\toprule
\textbf{Category} & \textbf{Type} & \textbf{Total} & \textbf{Real} & \textbf{Syn.} & \textbf{Variations} \\
\midrule
\multirow{5}{*}{Cartesian} 
& Vertical Bar         & 50 & 25 & 25 & 1. Single/Multi bars\\
& Horizontal Bar         & 50 & 25 & 25 & 1. Single/Multi bars\\
& Line        & 50 & 25 & 25 & 1. Vertex shape; 2. Line style \\
& Scatterplot     & 50 & 25 & 25 & 1. Vertex shape; 2. Labels \\
& Bubble      & 50 & 25 & 25 & 1. Label position \\
\cmidrule{1-6}
\multirow{5}{*}{Polar} 
& Pie          & 50 & 25 & 25 & --- \\
& Rose         & 50 &  0 & 50 & 1. Sector spacing \\
& Radar        & 50 &  0 & 50 & 1. Dimensions; 2. Fill \\
& Donut        & 50 &  0 & 50 & 1. Ring width \\
\bottomrule
\end{tabular}%
}
\end{table}


% \subsubsection{Evaluation Metrics}
% To comprehensively evaluate the performance of the proposed visual hint-guided reasoning system \textbf{VisHintPrompt} on coordinate prediction tasks, we adopt two complementary metrics.

% \textbf{(1) Mean Absolute Error (MAE).} This metric quantifies the semantic-level accuracy by measuring the average absolute difference between the predicted values and the ground truth values (e.g., chart values aligned with axes). For each data point, MAE is computed only on the available axis dimensions (e.g., Y-axis for bar charts, both X and Y for scatter plots). It is defined as:
% \begin{equation}
% \mathrm{MAE} = \frac{1}{D} \sum_{d \in \mathcal{A}} \frac{1}{N} \sum_{i=1}^{N} \left| \widehat{v}_i^{(d)} - v_i^{(d)} \right|
% \end{equation}
% where $\mathcal{A}$ denotes the set of available axes (e.g., $\{x,y\}$ for scatter plots, $\{y\}$ for bar charts), $D = |\mathcal{A}|$ is the number of available dimensions, and $N$ is the number of points.

% \textbf{(2) Relative Pixel Error (RE).} This metric evaluates visual alignment by measuring the average relative distance between predicted and ground-truth pixel coordinates, normalized by the image size. Similar to MAE, the RE is computed only on axes present in each task:
% \begin{equation}
% \mathrm{RE} = \frac{1}{D} \sum_{d \in \mathcal{A}} \frac{1}{N} \sum_{i=1}^{N} \frac{|\hat{p}_i^{(d)} - p_i^{(d)}|}{S^{(d)}}
% \end{equation}
% where $\hat{p}_i^{(d)}$ and $p_i^{(d)}$ denote the predicted and ground-truth pixel coordinates on dimension $d$, and $S^{(d)}$ is the image width or height, depending on the axis.

% These two metrics jointly evaluate semantic accuracy and spatial alignment, while accounting for the dimensionality of each chart type.
\subsubsection{\textbf{Evaluation Metrics}}

% Since each chart type provides a fixed and semantically aligned set of data
% points, predicted values correspond directly to ground-truth values without
% requiring cross-point matching. Following established evaluation practices in
% chart understanding research \cite{PlotQA,ChartQA,UniChart}, we adopt
% a range-normalized numerical error formulation that enables consistent comparison
% across charts with different value scales. The evaluation proceeds hierarchically
% from point-level errors to chart-level aggregation and finally to category-level
% summaries.

% Since each chart type provides a fixed set of semantically aligned data points, predicted values admit a direct one-to-one correspondence with ground-truth values, without requiring cross-point matching. 
Following the prior chart understanding research~\cite{PlotQA,ChartQA,UniChart}, we adopt a metric called
% range-normalized numerical error metric 
\textit{range-normalized error}
to enable consistent comparison across various charts with different value scales. The evaluation is conducted hierarchically, starting from \textit{point-wise} errors to aggregated
% \textit{point-level} errors as chart-level and further type-level errors.
\textit{chart-level} errors as well as \textit{type-level} errors.


\textbf{Point-wise Range-normalized Error (RNE).}
For a chart $c$ with ground truth values $\{v_i\}$ and predictions
$\{\hat{v}_i\}$, the range-normalized point-wise error is
\begin{equation}
e^{\mathrm{RNE}}_{c,i} =
\frac{\left| \hat{v}_i - v_i \right|}
     {v_{\max,c} - v_{\min,c}}.
\end{equation}
This normalization maps all errors into the $[0,1]$ interval, enabling fair
cross-chart comparison and aligning with prior work that evaluates chart value
prediction in normalized numeric space \cite{PlotQA,tang2023vistext}.

\textbf{Point-wise Relative Error (RE).}
To complement range normalization with a scale-sensitive metric, we also report
the relative error with respect to the ground truth value:
\begin{equation}
e^{\mathrm{RE}}_{c,i} =
\frac{\left| \hat{v}_i - v_i \right|}
     {\max\!\left(\left| v_i \right|, \epsilon\right)},
\end{equation}
where $\epsilon$ is a small constant to avoid division by zero. This metric
captures the relative deviation from the true value and is particularly
informative when charts contain values with different magnitudes.

\textbf{Chart-level Error.}
For a given point-wise error metric $m \in \{\mathrm{RNE}, \mathrm{RE}\}$,
the chart-level error is obtained by averaging the point-level errors:
\begin{equation}
E^{(m)}_c = \frac{1}{K_c} \sum_{i=1}^{K_c} e^{(m)}_{c,i},
\end{equation}
where $K_c$ denotes the number of data points in chart $c$. This produces a
single error measure per chart that reflects overall numerical consistency
under metric $m$.

\textbf{Type-level Error.}
Since our evaluation is conducted independently for each chart type, the final
score for a chart type is computed, for each metric $m$, by averaging the
chart-level errors over all charts of that type (e.g., bar chart and scatterplot) in our testing dataset: 
% \wy{category or type? Make it consistent throughout the paper!}
\begin{equation}
E^{(m)}_{\mathrm{type}} =
\frac{1}{C_{\mathrm{type}}}
\sum_{c \in \mathrm{type}} E^{(m)}_c,
\end{equation}
% where $C_{\mathrm{type}}$ is the number of charts of that type. This
% type-specific aggregation follows the reporting protocols of recent unified
% chart understanding models \cite{UniChart, han2023chartllama} and avoids
% cross-type mixing.
where $C_{\mathrm{type}}$ denotes the number of charts of that type. We report type-specific aggregates following recent unified chart understanding models \cite{UniChart, han2023chartllama} to avoid cross-type mixing.

Overall, this point–chart–type hierarchy, combined with both range-normalized
(RNE) and relative-to-ground-truth (RE) errors, provides a scale-invariant,
type-consistent, and literature-aligned evaluation of numerical inference
performance for \toolName{}.







\subsubsection{\textbf{Model Selection}}
To evaluate the model-agnostic generality of \toolName{}, we apply our
prompting strategy to a diverse set of MLLMs spanning both
closed-source and open-source systems. This design allows us to examine whether
structured visual hints consistently enhance numeric inference ability across
heterogeneous architectures without any fine-tuning or model-specific
adaptation.

\paragraph{Closed-source MLLMs}
% \wy{closed-source? The more common description is proprietary?}
We evaluate \toolName{} across three closed-source MLLMs with advanced visual–language capabilities.
\begin{itemize}
    \item \textbf{GPT-4o} — a general-purpose vision–language model known for its
          robust perception and reasoning.
    \item \textbf{Gemini~2.0 Flash} — an efficient and high-throughput model with
          competitive visual understanding.
    \item \textbf{Gemini~2.5 Flash} — an upgraded version with improved
          fine-grained spatial reasoning.
\end{itemize}

\paragraph{Open-source MLLMs}
We further evaluate two representative open-source models covering a wide range
of parameter scales and architectural families:
\begin{itemize}
    \item \textbf{InternVL3-78B} — a large, high-capacity open-source VLM with
          strong visual recognition.
    \item \textbf{Pixtral-12B-2409} — a transformer-based multimodal model with
          competitive image understanding.
    % \item \textbf{GLM-4.6V-Flash} — a lightweight model suitable for
    %       evaluating low-resource settings.
    
    % \item \textbf{Qwen2.5-VL-32B-Instruct} — a stronger variant offering improved
    %       geometric and numerical perception.
\end{itemize}

\paragraph{Rationale}
Our focus is to examine how much numeric extraction accuracy can be gained
\emph{without} any training, finetuning, or synthetic chart supervision. Modern
MLLMs already possess latent spatial and geometric reasoning capabilities, and
\toolName{} provides structured chart-specific visual hints that enable these
capabilities to be more effectively activated and utilized.
% \paragraph{Exclusion of chart-specific and OCR methods}
% \pyl{Conflict with the OCR method mentioned earlier in III.A.1)‘Alignment of tick values to pixel positions’}}
Chart-specific QA models (e.g., PlotQA, ChartQA, UniChart) rely on synthetic
training and support only a narrow set of chart families, making them
incompatible with our full-range numeric inference setting. 
% OCR-based pipelines are also excluded, as they require explicit printed numerical labels and cannot operate on most charts in our benchmark.

Overall, this model selection covers a comprehensive spectrum of MLLMs and
provides a rigorous testbed for assessing the generality and effectiveness of
\toolName{}.






\subsubsection{\textbf{Implementation Details}}


For closed-source
models, we access GPT-4o, Gemini-2.0-Flash, and Gemini-2.5-Flash via their
official APIs; GPT-4o uses deterministic decoding (temperature~$=0$), while the
Gemini models follow their default inference settings. For the open-source model Pixtral-12B-2409, inference is performed locally on a BI-V150 GPU server using the official released implementation, without any parameter modification. In contrast, InternVL3-78B 
% and GLM-4.6V-Flash
% \pyl{Is GLM omitted here?} 
is accessed via their official APIs under default inference settings. All prompts follow fixed templates for each elicitation component, and each model prediction is parsed through a strict JSON schema.
% with one retry if a malformed response is encountered. 
% \pyl{Retrying without modifying the input while setting the temperature to 0 is illogical, as the output will remain fixed and retain the original error. Instead, a proprietary closed-source LLM with non-zero temperature (and low formatting error propensity) is deployed for output format calibration.}
We did not perform any prompt tuning or model-specific adaptation beyond these fixed templates. The full implementation code is publicly available at \href{https://github.com/tu4nzii/VisHintPrompt_code}{https://github.com/tu4nzii/VisHintPrompt\_code}.







